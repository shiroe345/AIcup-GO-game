{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb173248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, Flatten, Dense, Activation, Softmax, MaxPooling2D \n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62384345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76ada379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "# if gpus:\n",
    "#     # Restrict TensorFlow to only use the first GPU\n",
    "#     try:\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, False)\n",
    "#             tf.config.experimental.set_virtual_device_configuration(\n",
    "#                 gpu,\n",
    "#                 [\n",
    "#                     tf.config.experimental.VirtualDeviceConfiguration(\n",
    "#                         memory_limit=4096  # set your limit\n",
    "#                     )\n",
    "#                 ],\n",
    "#             )\n",
    "#         tf.config.experimental.set_visible_devices(gpus[0], \"GPU\")\n",
    "#         logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "#         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "#     except RuntimeError as e:\n",
    "#         # Visible devices must be set before GPUs have been initialized\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0c3979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "\n",
    "if len(physical_devices) == 0:\n",
    "    print(\"No GPU devices found. Using CPU.\")\n",
    "else:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd95ed0c",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "Open **dan_train.csv** file and split the games into a list.\n",
    "Every row of csv: `DL0000000001,B,B[pd],W[dp],B[pp],W[dc],B[de],...`. \n",
    "\n",
    "Columns are:\n",
    "\n",
    "    1. DL0000000001: Game ID\n",
    "    2. B: Player's color\n",
    "    3-... : Moves\n",
    "    \n",
    "We cropped only the moves to game list as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "381b160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = open('./Training Dataset/dan_train.csv').read().splitlines()\n",
    "# gm = [i.split(',',1)[-1] for i in df]\n",
    "# for i in range(len(gm)):\n",
    "#     if gm[i][0] != gm[i][-5]:\n",
    "#         gm[i] = gm[i][0:-6]\n",
    "# games = [i.split(',',1)[-1] for i in gm]\n",
    "# print(games[0])\n",
    "# random.shuffle(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f8872fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = open('./Training Dataset/dan_train.csv').read().splitlines()\n",
    "# df = open('./CSVs/Tutorial_dan_train.csv').read().splitlines()\n",
    "games = [i.split(',',2)[-1] for i in df]\n",
    "random.shuffle(games)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58532b01",
   "metadata": {},
   "source": [
    "Create a dictionary to convert the coordinates from characters to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d48bb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227.16873003194888 379 107 0\n"
     ]
    }
   ],
   "source": [
    "avg = 0\n",
    "mn = 12345\n",
    "mx = 0\n",
    "count = 0\n",
    "zeros = 0\n",
    "for g in games:\n",
    "    moves_list = g.split(',')\n",
    "    if len(moves_list) <= 10:\n",
    "        zeros += 1\n",
    "    avg += len(moves_list)-1\n",
    "    mn = min(mn, len(moves_list)-1)\n",
    "    mx = max(mx, len(moves_list)-1)\n",
    "    count += 1\n",
    "print(avg/len(games), mx, mn, zeros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "496585f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'b': 1,\n",
       " 'c': 2,\n",
       " 'd': 3,\n",
       " 'e': 4,\n",
       " 'f': 5,\n",
       " 'g': 6,\n",
       " 'h': 7,\n",
       " 'i': 8,\n",
       " 'j': 9,\n",
       " 'k': 10,\n",
       " 'l': 11,\n",
       " 'm': 12,\n",
       " 'n': 13,\n",
       " 'o': 14,\n",
       " 'p': 15,\n",
       " 'q': 16,\n",
       " 'r': 17,\n",
       " 's': 18}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = 'abcdefghijklmnopqrs'\n",
    "coordinates = {k:v for v,k in enumerate(chars)}\n",
    "chartonumbers = {k:v for k,v in enumerate(chars)}\n",
    "coordinates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92277370",
   "metadata": {},
   "source": [
    "We decided to build a DCNN model in this tutorial. We create data samples by using every move in every game, meaning that the target is to predict the next move by feeding the previous state of the table in every game for every move. Therefore, we can collect much more data samples from games.\n",
    "\n",
    "For the simplicity, we used 4 dimensional feature map to represent the data as below:\n",
    " 1. Positions of black stones: mark them as 1 and the rest of the table as 0\n",
    " 2. Positions of white stones: mark them as 1 and the rest of the table as 0\n",
    " 3. Empty areas of the table: mark the empty areas as 1 and occupied areas as 0\n",
    " 4. The last move in the table: mark the position of the last move as 1 and the rest as 0\n",
    " \n",
    "Target value is a number between 0-361(19\\*19). Later this will be one-hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c9c6f",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6a9c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_liberties(board):\n",
    "    board[:, :, 4] = np.zeros((19,19)) # initialize\n",
    "\n",
    "    for x in range(19):\n",
    "        for y in range(19):\n",
    "            if board[x, y, 2] == 0:  # 如果是空點，計算相鄰的空點數\n",
    "                for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n",
    "                    nx, ny = x + dx, y + dy\n",
    "                    if 0 <= nx < 19 and 0 <= ny < 19:\n",
    "                        if board[nx, ny, 2]:\n",
    "                            board[nx, ny, 4] += 1 # liberties += 1\n",
    "\n",
    "def dfs(x, i, j, visited, stack):\n",
    "    stack.append((i,j))\n",
    "    visited[i][j] = 1\n",
    "    liberty = x[i, j, 4]\n",
    "    color = 1\n",
    "    if x[i, j, 0]:\n",
    "        color = 0\n",
    "\n",
    "    for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n",
    "        nx, ny = i + dx, j + dy\n",
    "        if 0 <= nx < 19 and 0 <= ny < 19 and x[nx, ny, color] and not visited[nx][ny]:\n",
    "            liberty += dfs(x, nx, ny, visited, stack)\n",
    "\n",
    "    return liberty    \n",
    "    \n",
    "\n",
    "def calculate_connected(x):\n",
    "    visited = [[0] * 19 for _ in range(19)]\n",
    "\n",
    "    for i in range(19):\n",
    "        for j in range(19):\n",
    "            if not visited[i][j] and x[i, j, 2]:\n",
    "                stack = []\n",
    "                sum = dfs(x, i, j, visited, stack)\n",
    "                while stack: # while not empty\n",
    "                    row, col = stack.pop()\n",
    "                    x[row, col, 4] = sum\n",
    "\n",
    "def check_dead(x):\n",
    "    for i in range(19):\n",
    "        for j in range(19):\n",
    "            if x[i, j, 4] == 0 and ( x[i, j, 0] == 1 or x[i, j, 1] == 1 ):\n",
    "                x[i, j, 0] = 0\n",
    "                x[i, j, 1] = 0\n",
    "                x[i, j, 2] = 0\n",
    "                print(\"it works\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10fa7bb",
   "metadata": {},
   "source": [
    "### 5-layer version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e6ed8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def prepare_input(moves):\n",
    "#     x = np.zeros((19,19,5))\n",
    "#     for move in moves:\n",
    "#         color = (0 if move[0] == 'B' else 1)\n",
    "#         column = coordinates[move[2]]\n",
    "#         row = coordinates[move[3]]\n",
    "#         if color == 0:\n",
    "#             x[row,column,0] = 1\n",
    "#             x[row,column,2] = 1\n",
    "#         if color == 1:\n",
    "#             x[row,column,1] = 1\n",
    "#             x[row,column,2] = 1\n",
    "#     if moves:\n",
    "#         last_move_column = coordinates[moves[-1][2]]\n",
    "#         last_move_row = coordinates[moves[-1][3]]\n",
    "#         calculate_liberties(x)\n",
    "#         calculate_connected(x)\n",
    "#         check_dead(x)\n",
    "#         calculate_liberties(x)\n",
    "#         calculate_connected(x)\n",
    "#         x[last_move_row,last_move_column,3] = 1\n",
    "#     x[:,:,2] = np.where(x[:,:,2] == 0, 1, 0)\n",
    "#     return x\n",
    "\n",
    "# def prepare_label(move):\n",
    "#     column = coordinates[move[2]]\n",
    "#     row = coordinates[move[3]]\n",
    "#     return column*19+row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77676a04",
   "metadata": {},
   "source": [
    "### 4-layer version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9c6c1019",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def prepare_input(moves):\n",
    "#     x = np.zeros((19,19,5))\n",
    "#     for move in moves:\n",
    "#         color = (0 if move[0] == 'B' else 1)\n",
    "#         column = coordinates[move[2]]\n",
    "#         row = coordinates[move[3]]\n",
    "#         if color == 0:\n",
    "#             x[row,column,0] = 1\n",
    "#             x[row,column,2] = 1\n",
    "#         if color == 1:\n",
    "#             x[row,column,1] = 1\n",
    "#             x[row,column,2] = 1\n",
    "#     if moves:\n",
    "#         last_move_column = coordinates[moves[-1][2]]\n",
    "#         last_move_row = coordinates[moves[-1][3]]\n",
    "#         calculate_liberties(x)\n",
    "#         calculate_connected(x)\n",
    "#         check_dead(x)\n",
    "#         x[last_move_row,last_move_column,3] = 1\n",
    "#     x[:,:,2] = np.where(x[:,:,2] == 0, 1, 0)\n",
    "#     return x[:,:,:4]\n",
    "\n",
    "# def prepare_label(move):\n",
    "#     column = coordinates[move[2]]\n",
    "#     row = coordinates[move[3]]\n",
    "#     return column*19+row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4cb52b",
   "metadata": {},
   "source": [
    "### two layer version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50d95b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_input(moves):\n",
    "#     x = np.zeros((19,19,2))\n",
    "#     for move in moves:\n",
    "#         color = move[0]\n",
    "#         column = coordinates[move[2]]\n",
    "#         row = coordinates[move[3]]\n",
    "#         if color == 'B':\n",
    "#             x[row,column,0] = 1\n",
    "#         if color == 'W':\n",
    "#             x[row,column,0] = 2\n",
    "#     if moves:\n",
    "#         last_move_column = coordinates[moves[-1][2]]\n",
    "#         last_move_row = coordinates[moves[-1][3]]\n",
    "#         x[last_move_row,last_move_column,1] = 1\n",
    "#     return x\n",
    "\n",
    "# def prepare_label(move):\n",
    "#     column = coordinates[move[2]]\n",
    "#     row = coordinates[move[3]]\n",
    "#     return column*19+row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e16bce",
   "metadata": {},
   "source": [
    "### normal version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0adb423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(moves):\n",
    "    x = np.zeros((19,19,4))\n",
    "    for move in moves:\n",
    "        color = move[0]\n",
    "        column = coordinates[move[2]]\n",
    "        row = coordinates[move[3]]\n",
    "        if color == 'B':\n",
    "            x[row,column,0] = 1\n",
    "            x[row,column,2] = 1\n",
    "        if color == 'W':\n",
    "            x[row,column,1] = 1\n",
    "            x[row,column,2] = 1\n",
    "    if moves:\n",
    "        last_move_column = coordinates[moves[-1][2]]\n",
    "        last_move_row = coordinates[moves[-1][3]]\n",
    "        x[last_move_row,last_move_column,3] = 1\n",
    "    x[:,:,2] = np.where(x[:,:,2] == 0, 1, 0)\n",
    "    return x\n",
    "\n",
    "def prepare_label(move):\n",
    "    if len(move) < 2:\n",
    "        print(move)\n",
    "        return 0\n",
    "    column = coordinates[move[2]]\n",
    "    row = coordinates[move[3]]\n",
    "    return column*19+row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd26ffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "\n",
    "# def prepare_input(moves):\n",
    "#     x = np.zeros((19,19,3))\n",
    "#     for move in moves:\n",
    "#         color = move[0]\n",
    "#         column = coordinates[move[2]]\n",
    "#         row = coordinates[move[3]]\n",
    "#         if color == 'B':\n",
    "#             x[row,column,0] = 1\n",
    "#         if color == 'W':\n",
    "#             x[row,column,0] = 2\n",
    "#     if moves:\n",
    "#         last_move_column = coordinates[moves[-1][2]]\n",
    "#         last_move_row = coordinates[moves[-1][3]]\n",
    "#         x[last_move_row,last_move_column,1] = 1\n",
    "#     x = np.pad(x, ((102, 103), (102, 103), (0, 0)), mode='constant')\n",
    "#     x = cv2.resize(x, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "#     return x\n",
    "\n",
    "# def prepare_label(move):\n",
    "#     column = coordinates[move[2]]\n",
    "#     row = coordinates[move[3]]\n",
    "#     return column*19+row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46403360",
   "metadata": {},
   "source": [
    "The code below is run for baseline model only by using only the first 500 games from the dataset. You might need to create a data generator to use complete dataset. Otherwise your RAM might not enough to store all (If you run the code on free version of Google Colab, it will crash above 500 game samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b048ff",
   "metadata": {},
   "source": [
    "Dataset splitting: 90% Training, 10% validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9c5de9",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "### Simple DCNN Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7fa9006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6735905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !taskkill /F /PID 10952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd67e439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 8564), started 2 days, 7:57:25 ago. (Use '!kill 8564' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-29f8fdc1dc2bc3f8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-29f8fdc1dc2bc3f8\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=path/to/logs --host localhost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "208834da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = Input(shape=(19, 19, 4))\n",
    "\n",
    "    outputs = Conv2D(kernel_size=7, filters=64, padding='same', activation='relu')(inputs)\n",
    "    outputs = Conv2D(kernel_size=7, filters=64, padding='same', activation='relu')(outputs)\n",
    "    outputs = Conv2D(kernel_size=5, filters=64, padding='same', activation='relu')(outputs)\n",
    "    outputs = Conv2D(kernel_size=5, filters=64, padding='same', activation='relu')(outputs)\n",
    "    outputs = Conv2D(kernel_size=3, filters=64, padding='same', activation='relu')(outputs)\n",
    "    outputs = Conv2D(kernel_size=3, filters=1, padding='same', activation='relu')(outputs)\n",
    "    outputs = Flatten()(outputs)\n",
    "    outputs = Dense(256, activation='relu')(outputs)\n",
    "    outputs = Dropout(0.3)(outputs)\n",
    "    outputs = Dense(361, activation='softmax')(outputs)  # Adjust the number of units based on your output space\n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    opt = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1107ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model1():\n",
    "    inputs = Input(shape=(19, 19, 4))\n",
    "    \n",
    "    x = Conv2D(kernel_size=7, filters=64, padding='same', activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)  # Add BatchNormalization\n",
    "    x = Conv2D(kernel_size=7, filters=64, padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(kernel_size=5, filters=64, padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(kernel_size=5, filters=64, padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(kernel_size=3, filters=64, padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(kernel_size=3, filters=1, padding='same', activation='relu')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "    policy_head = Dense(361, activation='softmax', name='policy')(x)\n",
    "    value_head = Dense(1, activation='tanh', name='value')(x)\n",
    "\n",
    "    model = Model(inputs, [policy_head, value_head])\n",
    "\n",
    "    opt = Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss={'policy': 'categorical_crossentropy', 'value': 'mean_squared_error'},\n",
    "                  metrics={'policy': 'accuracy', 'value': 'mae'})\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5020cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model2():\n",
    "    inputs = Input(shape=(19, 19, 4))\n",
    "\n",
    "    outputs = Conv2D(kernel_size=7, filters=64, padding='same', activation='relu')(inputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Conv2D(kernel_size=7, filters=64, padding='same', activation='relu')(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Conv2D(kernel_size=5, filters=64, padding='same', activation='relu')(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Conv2D(kernel_size=5, filters=64, padding='same', activation='relu')(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Conv2D(kernel_size=3, filters=64, padding='same', activation='relu')(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Conv2D(kernel_size=3, filters=1, padding='same', activation='relu')(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    \n",
    "    outputs = Flatten()(outputs)\n",
    "    outputs = Dense(256, activation='relu')(outputs)\n",
    "    outputs = Dropout(0.5)(outputs)\n",
    "    outputs = Dense(361, activation='softmax')(outputs)  # Adjust the number of units based on your output space\n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    opt = Adam(learning_rate=0.001)  # Adjust the learning rate\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='categorical_crossentropy',  # Change to sparse_categorical_crossentropy\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd14898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "def create_ResNetmodel(img_shape, num_classes):\n",
    "    model_resnet = ResNet50(include_top=False,weights='imagenet')\n",
    "    # Make resnet50 model layers as non trainable\n",
    "    for layer in model_resnet.layers:\n",
    "        layer.trainable = False\n",
    "    img_input = Input(shape=img_shape)\n",
    "    img_model_resnet = model_resnet(img_input)\n",
    "    x = Flatten(name='flatten')(img_model_resnet)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "    modified_model = Model(inputs=img_input, outputs=x)\n",
    "    modified_model.compile(loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam', metrics=['acc'])\n",
    "    return modified_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "991bfaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, Input\n",
    "\n",
    "def basic_block(x, filters, stride=1):\n",
    "    identity = x\n",
    "    x = Conv2D(filters, kernel_size=(3, 3), strides=stride, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters, kernel_size=(3, 3), strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if stride != 1:\n",
    "        identity = Conv2D(filters, kernel_size=(1, 1), strides=stride, padding='same')(identity)\n",
    "        identity = BatchNormalization()(identity)\n",
    "    x = Add()([x, identity])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def resnet34(input_shape=(224, 224, 3)):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(64, kernel_size=(7, 7), strides=2, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Stack basic blocks (adjust the number of blocks as needed)\n",
    "    x = basic_block(x, filters=64)\n",
    "    x = basic_block(x, filters=64)\n",
    "    x = basic_block(x, filters=64)\n",
    "\n",
    "    x = basic_block(x, filters=128, stride=2)\n",
    "    x = basic_block(x, filters=128)\n",
    "    x = basic_block(x, filters=128)\n",
    "    x = basic_block(x, filters=128)\n",
    "\n",
    "    x = basic_block(x, filters=256, stride=2)\n",
    "    x = basic_block(x, filters=256)\n",
    "    x = basic_block(x, filters=256)\n",
    "    x = basic_block(x, filters=256)\n",
    "    x = basic_block(x, filters=256)\n",
    "    x = basic_block(x, filters=256)\n",
    "\n",
    "    x = basic_block(x, filters=512, stride=2)\n",
    "    x = basic_block(x, filters=512)\n",
    "    x = basic_block(x, filters=512)\n",
    "\n",
    "    # Add global average pooling and output layer\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = Dense(361, activation='softmax')(x)  # Adjust num_classes\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    opt = Adam(learning_rate=0.001)  # Adjust the learning rate\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='categorical_crossentropy',  # Change to sparse_categorical_crossentropy\n",
    "                  metrics=['accuracy'])\n",
    "                  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ba5842ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 19, 19, 4)]       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 19, 19, 64)        12608     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 19, 19, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 19, 19, 64)        200768    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 19, 19, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 19, 19, 64)        102464    \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 19, 19, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 19, 19, 64)        102464    \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 19, 19, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 19, 19, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 19, 19, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 19, 19, 1)         577       \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 19, 19, 1)        4         \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 361)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               92672     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 361)               92777     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 642,542\n",
      "Trainable params: 641,900\n",
      "Non-trainable params: 642\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "model = create_model2()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a66e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_model2()\n",
    "# # model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ec53e",
   "metadata": {},
   "source": [
    "## data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "993f365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import set_session\n",
    "from keras.backend import clear_session\n",
    "from keras.backend import get_session\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import gc\n",
    "import math\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "773128b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reset keras function from: https://github.com/keras-team/keras/issues/12625\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    # try:\n",
    "    #     del classifier # this is from global space - change this as you need\n",
    "    # except:\n",
    "    #     pass\n",
    "    gc.collect()\n",
    "    # print(\"Garbage Collected: \" + gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=1, visible_device_list=\"0\")\n",
    "    config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n",
    "    set_session(tf.compat.v1.Session(config=config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f467a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming prepare_input and prepare_label functions are defined elsewhere\n",
    "\n",
    "def data_generator(games, batch_size):\n",
    "    for batch_start in range(0, len(games), batch_size):\n",
    "\n",
    "        batch_end = batch_start + batch_size\n",
    "        batch_games = games[batch_start:batch_end]\n",
    "        x = []\n",
    "        y = []\n",
    "        for game in batch_games:\n",
    "            moves_list = game.split(',')\n",
    "            for count, move in enumerate(moves_list):\n",
    "                x.append(prepare_input(moves_list[:count]))\n",
    "                y.append(prepare_label(moves_list[count]))\n",
    "\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        # batch_number = batch_start/batch_size\n",
    "        # np.save(f'inputs/x{batch_number}.npy', x)\n",
    "        # np.save(f'inputs/y{batch_number}.npy', y)\n",
    "\n",
    "        \n",
    "\n",
    "        # x = np.load(f'inputs/x{batch_number}.npy')\n",
    "        # y = np.load(f'inputs/y{batch_number},npy')\n",
    "        \n",
    "        y_one_hot = tf.one_hot(y, depth=19*19)\n",
    "\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x, y_one_hot.numpy(), test_size=0.10)\n",
    "\n",
    "        yield (x_train, y_train, x_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "39cbd6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MCTSNode:\n",
    "#     def __init__(self, state):\n",
    "#         self.state = state\n",
    "#         self.parent = None\n",
    "#         self.children = {}\n",
    "#         self.visits = 0\n",
    "#         self.score = 0\n",
    "\n",
    "# def mcts_predict(board_states):\n",
    "#     root = MCTSNode(board_states)  # 初始化 MCTS 根節點\n",
    "#     number_of_simulation = 500\n",
    "#     EXPLORATION_CONST = 5\n",
    "    \n",
    "#     # MCTS 搜索\n",
    "#     for i in range(number_of_simulation):\n",
    "#         node = root\n",
    "#         state = board_states.copy()  # 假設有一個 copy 方法來複製棋盤狀態\n",
    "#         # 選擇節點，根據 UCB 算法\n",
    "#         while node.children:\n",
    "#             # UCB1 公式\n",
    "#             child = max(node.children.items(), key=lambda c: c[1].score / (c[1].visits + 1.0) + EXPLORATION_CONST * np.sqrt(np.log(node.visits + 1.0) / (c[1].visits + 1.0)))\n",
    "#             state = child[0]  # 假設這是下一步的狀態\n",
    "#             node = child[1]\n",
    "        \n",
    "#         # 展開節點，根據 MCTS 算法\n",
    "#         if not state.is_game_over():\n",
    "#             legal_moves = state.get_legal_moves()  # 假設有一個 get_legal_moves 方法來獲取合法走法\n",
    "#             for move in legal_moves:\n",
    "#                 new_state = move  # 假設這是一個新的棋盤狀態\n",
    "#                 node.children[move] = MCTSNode(new_state)\n",
    "        \n",
    "#         # 模擬遊戲\n",
    "#         rollout_state = state.copy()\n",
    "#         while not rollout_state.is_game_over():\n",
    "#             legal_moves = rollout_state.get_legal_moves()\n",
    "#             move = random.choice(legal_moves)\n",
    "#             rollout_state = move  # 假設這是下一步的狀態\n",
    "        \n",
    "#         # 回溯更新節點\n",
    "#         while node is not None:\n",
    "#             node.visits += 1\n",
    "#             node.score += rollout_state.get_score()  # 假設有一個 get_score 方法來獲取當前狀態得分\n",
    "#             node = node.parent\n",
    "    \n",
    "#     # 根據 MCTS 搜索結果返回預測的下一步棋\n",
    "#     best_child = max(root.children.items(), key=lambda c: c[1].visits)\n",
    "#     return best_child[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 512  # 調整批次大小\n",
    "# batchs = math.ceil(len(games) / batch_size)\n",
    "\n",
    "# # 設置 TensorBoard 回調\n",
    "# tensorboard_callback = TensorBoard(log_dir=f\"dan_logs/{time.time()}\", histogram_freq=1)\n",
    "\n",
    "# for epoch in range(1, 2):\n",
    "#     print(\"epoch\", epoch)\n",
    "#     batch_count = 1\n",
    "    \n",
    "#     for x_train, y_train, x_val, y_val in data_generator(games, batch_size):\n",
    "#         print(f\"{batch_count}/{batchs}\")\n",
    "\n",
    "#         # MCTS 預測下一步棋步\n",
    "#         next_move = mcts_predict(x_train)\n",
    "\n",
    "#         # 將 MCTS 預測的棋步轉換為模型所需格式\n",
    "#         mcts_input = prepare_input(next_move)\n",
    "#         mcts_label = prepare_label(next_move)\n",
    "\n",
    "#         # 添加 MCTS 預測的棋步到訓練數據\n",
    "#         x_train = np.append(x_train, [mcts_input], axis=0)\n",
    "#         y_train = np.append(y_train, [mcts_label], axis=0)\n",
    "\n",
    "#         # 訓練模型\n",
    "#         history = model.fit(\n",
    "#             x=x_train, \n",
    "#             y=y_train,\n",
    "#             batch_size=1024,\n",
    "#             epochs=1,\n",
    "#             validation_data=(x_val, y_val),\n",
    "#             callbacks=[tensorboard_callback]\n",
    "#         )\n",
    "\n",
    "#         if batch_count % 10 == 0:\n",
    "#             model.save(f\"./models/dan_{batch_count}_{history.history['val_accuracy'][0]:.5f}_{history.history['val_loss'][0]:.5f}.h5\")\n",
    "\n",
    "#         batch_count += 1\n",
    "#         reset_keras()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0c4727de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "(115530, 19, 19, 4)\n",
      "1/196\n",
      "102/102 [==============================] - 21s 177ms/step - loss: 5.8406 - accuracy: 0.0056 - val_loss: 5.8699 - val_accuracy: 0.0029\n",
      "(117565, 19, 19, 4)\n",
      "2/196\n",
      " 28/104 [=======>......................] - ETA: 10s - loss: 5.7194 - accuracy: 0.0096"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_train, y_train, x_val, y_val \u001b[38;5;129;01min\u001b[39;00m data_generator(games, batch_size):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatchs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add TensorBoard callback here\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     23\u001b[0m         model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/dan2_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 512  # Adjust this if needed\n",
    "batchs = math.ceil(len(games) / batch_size)\n",
    "\n",
    "# Set up TensorBoard callback\n",
    "tensorboard_callback = TensorBoard(log_dir=f\"dan_logs/{time.time()}\", histogram_freq=1)\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    print(\"epoch\", epoch)\n",
    "    batch_count = 1\n",
    "    \n",
    "    for x_train, y_train, x_val, y_val in data_generator(games, batch_size):\n",
    "        print(f\"{batch_count}/{batchs}\")\n",
    "        history = model.fit(\n",
    "            x=x_train, \n",
    "            y=y_train,\n",
    "            batch_size=1024,\n",
    "            epochs=1,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks=[tensorboard_callback]  # Add TensorBoard callback here\n",
    "        )\n",
    "\n",
    "        if batch_count % 10 == 0:\n",
    "            model.save(f\"./models/dan2_{batch_count}_{history.history['val_accuracy'][0]:.5f}_{history.history['val_loss'][0]:.5f}.h5\")\n",
    "            # model.save(f\"./models/dan_{batch_count}_{history.history['val_policy_accuracy'][0]:.5f}_{history.history['val_loss'][0]:.5f}.h5\")\n",
    "\n",
    "        batch_count += 1\n",
    "        reset_keras()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d2bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: [0.47193917632102966]\n"
     ]
    }
   ],
   "source": [
    "print(\"val_acc:\", history.history['val_accuracy'])\n",
    "model.save(f'./models/model_dan2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484be28d",
   "metadata": {},
   "source": [
    "## ALL DONE!\n",
    "\n",
    "For using the model and creating a submission file, follow the notebook **Create Public Upload CSV.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7fafaa",
   "metadata": {},
   "source": [
    "# End of Tutorial\n",
    "\n",
    "You are free to use more modern NN architectures, a better pre-processing, feature extraction methods to achieve much better accuracy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
